{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pointed-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-plaza",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unknown-union",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting video stream\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n",
      "(1, 1, 200, 7)\n"
     ]
    }
   ],
   "source": [
    "#returns the value of location of square block and prediction\n",
    "def detectpredict(frame, faceNet, maskNet):\n",
    "    #left\n",
    "    (h,w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (224,224),(104.0, 177.0, 123.0))      #pixels size 224x224 and mean subtracting\n",
    "     #obtaining face detection                            \n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "    print(detections.shape)                                #shape of area detected\n",
    "    \n",
    "    # faces, their locations and prediction from facemask network\n",
    "    faces=[]\n",
    "    locs=[]\n",
    "    preds=[]\n",
    "    \n",
    "    for i in range(0, detections.shape[2]):\n",
    "        \n",
    "        confidence = detections[0,0,i,2]\n",
    "        #filter weak detections\n",
    "        if confidence > 0.5:\n",
    "            box= detections[0,0,i,3:7]* np.array([w,h,w,h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            (startX, startY)=(max(0,startX), max(0,startY))\n",
    "            (endX, endY)= (min(w-1, endX), min(h-1, endY))\n",
    "            \n",
    "            #extract the roi, convert from bgr to rgb\n",
    "            #set size 224,224\n",
    "            face=frame[startX:endX, startY:endY]\n",
    "            face=cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            face=cv2.resize(face, (224,224))\n",
    "            face=img_to_array(face)\n",
    "            face=preprocess_input(face)\n",
    "            \n",
    "            faces.append(face)\n",
    "            locs.append((startX,startY,endX,endY))\n",
    "            \n",
    "    if len(faces)>0:\n",
    "        faces=np.array(faces, dtype=\"float32\")\n",
    "        preds=maskNet.predict(faces, batch_size=32)\n",
    "        \n",
    "    return (locs,preds)\n",
    "\n",
    "prototxtPath = r\"face_detector\\deploy.prototxt\"\n",
    "weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "maskNet = load_model(\"mask_detector.model\")\n",
    "\n",
    "print(\"starting video stream\")\n",
    "vs = VideoStream(src=0).start()         #src=0 for primary camera\n",
    "\n",
    "while True:\n",
    "    #frames from the video and resize to make max width 400\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "    \n",
    "    (locs, preds) = detectpredict(frame, faceNet, maskNet)\n",
    "     \n",
    "    for(box, pred) in zip(locs,preds):\n",
    "        (startX,startY,endX,endY)=box\n",
    "        (mask,without_mask)=pred\n",
    "        \n",
    "        label=\"mask present\" if mask > without_mask else \"No Mask\"\n",
    "        color= (0,255,0) if label==\"mask present\" else (0,0,225)  #BGR\n",
    "        \n",
    "        label = \"{}:{:.2f}%\".format(label, max(mask, without_mask)*100)         #label, %prediction using .format\n",
    "        \n",
    "        #display label and box\n",
    "        cv2.putText(frame, label, (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)   # cv2.putText(image, text, org, font, fontScale, color[], thickness[])\n",
    "        cv2.rectangle(frame, (startX,startY),(endX,endY),color,2)       #above color coding\n",
    "        \n",
    "    #show frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) % 0xFF\n",
    "    \n",
    "    if key==ord(\"q\"):    #end\n",
    "        break\n",
    "\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-option",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-point",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
